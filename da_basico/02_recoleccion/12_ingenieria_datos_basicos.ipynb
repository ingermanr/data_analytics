{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2916d35",
   "metadata": {},
   "source": [
    "<p style=\"margin: 5px 0 0 0; color: #666;\"><em>Desarrollado con Claude - Anthropic</em></p>\n",
    "\n",
    "# 12. Conceptos de Ingenier√≠a de Datos B√°sicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d65151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì M√≥dulos importados correctamente\n",
      "Fecha actual: 2026-02-21 22:22:15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "import pyarrow\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pre-registrar tipos de extensi√≥n pyarrow para evitar conflictos al re-ejecutar celdas\n",
    "try:\n",
    "    import pandas.core.arrays.arrow.extension_types\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"‚úì M√≥dulos importados correctamente\")\n",
    "print(f\"Fecha actual: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6ff16",
   "metadata": {},
   "source": [
    "## Introducci√≥n a la Ingenier√≠a de Datos\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "La **ingenier√≠a de datos** es la disciplina que se encarga de dise√±ar, construir y mantener los sistemas e infraestructuras necesarios para recopilar, almacenar, transformar y distribuir datos. Es el puente entre las fuentes de datos en bruto y los equipos de an√°lisis que extraen valor de ellos.\n",
    "\n",
    "**Ingeniero de Datos vs Analista de Datos:**\n",
    "\n",
    "| Aspecto | Ingeniero de Datos | Analista de Datos |\n",
    "|---|---|---|\n",
    "| Enfoque | Infraestructura y arquitectura | Estad√≠stica y visualizaci√≥n |\n",
    "| Construye | Pipelines y automatizaciones | Insights y reportes |\n",
    "| Tecnolog√≠as | Spark, Airflow, Kafka | Python, SQL, Power BI |\n",
    "| Objetivo | Datos disponibles y confiables | Valor y decisiones basadas en datos |\n",
    "\n",
    "**Habilidades compartidas:** SQL avanzado, Python, comprensi√≥n de datos, resoluci√≥n de problemas y documentaci√≥n.\n",
    "\n",
    "**Ciclo de vida de los datos:**\n",
    "\n",
    "1. **Fuentes de datos** ‚Üí Bases de datos, APIs, archivos, streaming\n",
    "2. **Ingesta** ‚Üí Extracci√≥n batch o streaming\n",
    "3. **Almacenamiento crudo** ‚Üí Data Lake con formatos eficientes (Parquet, Avro)\n",
    "4. **Transformaci√≥n** ‚Üí Limpieza, normalizaci√≥n, enriquecimiento\n",
    "5. **Almacenamiento procesado** ‚Üí Data Warehouse optimizado para an√°lisis\n",
    "6. **Consumo** ‚Üí Dashboards, ML, APIs, an√°lisis ad-hoc\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Comprender los fundamentos de ingenier√≠a de datos sirve para:\n",
    "\n",
    "- **Entender de d√≥nde vienen los datos** y c√≥mo llegan hasta las herramientas de an√°lisis\n",
    "- **Ser un analista m√°s vers√°til** con capacidad de gestionar el ciclo completo de los datos\n",
    "- **Manejar datos de forma m√°s eficiente** eligiendo formatos y arquitecturas adecuadas\n",
    "- **Automatizar tareas repetitivas** como reportes, descargas y limpiezas peri√≥dicas\n",
    "- **Colaborar mejor con ingenieros de datos** al compartir vocabulario t√©cnico\n",
    "- **Crear pipelines b√°sicos** que automatizan la recolecci√≥n y transformaci√≥n de datos\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, simulamos un mini flujo de datos que recorre las etapas del ciclo de vida: desde la extracci√≥n hasta el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8acccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                 EJEMPLO: MINI CICLO DE VIDA DE DATOS                 \n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ FUENTE - Datos crudos del sistema:\n",
      "   id producto precio       fecha\n",
      "0   1   Laptop   1200  2024-01-15\n",
      "1   2    mouse     25  2024/01/16\n",
      "2   3  TECLADO    N/A  15-01-2024\n",
      "3   4  Monitor    350  2024-01-17\n",
      "4   5      NaN     80  2024-01-18\n",
      "   Problemas: may√∫sculas inconsistentes, nulo, tipo incorrecto, fechas mixtas\n",
      "\n",
      "2Ô∏è‚É£ TRANSFORMACI√ìN - Limpieza y normalizaci√≥n:\n",
      "   id     producto  precio      fecha\n",
      "0   1       Laptop  1200.0 2024-01-15\n",
      "1   2        Mouse    25.0 2024-01-16\n",
      "2   3      Teclado     0.0 2024-01-15\n",
      "3   4      Monitor   350.0 2024-01-17\n",
      "4   5  Desconocido    80.0 2024-01-18\n",
      "   Tipos: {'id': dtype('int64'), 'producto': <StringDtype(na_value=nan)>, 'precio': dtype('float64'), 'fecha': dtype('<M8[us]')}\n",
      "\n",
      "3Ô∏è‚É£ CONSUMO - An√°lisis listo para dashboards:\n",
      "   Total productos: 5\n",
      "   Precio promedio: $331.00\n",
      "   Rango de fechas: 2024-01-15 a 2024-01-18\n",
      "\n",
      "‚úÖ Datos pasaron de crudos a listos para an√°lisis\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO: MINI CICLO DE VIDA DE DATOS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. FUENTE: Simular datos crudos de un sistema transaccional\n",
    "print(\"\\n1Ô∏è‚É£ FUENTE - Datos crudos del sistema:\")\n",
    "datos_crudos = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'producto': ['Laptop', 'mouse', 'TECLADO', 'Monitor', None],\n",
    "    'precio': [1200, 25, 'N/A', 350, 80],\n",
    "    'fecha': ['2024-01-15', '2024/01/16', '15-01-2024', '2024-01-17', '2024-01-18']\n",
    "})\n",
    "print(datos_crudos)\n",
    "print(f\"   Problemas: may√∫sculas inconsistentes, nulo, tipo incorrecto, fechas mixtas\")\n",
    "\n",
    "# 2. TRANSFORMACI√ìN: Limpiar y normalizar\n",
    "print(\"\\n2Ô∏è‚É£ TRANSFORMACI√ìN - Limpieza y normalizaci√≥n:\")\n",
    "datos_limpios = datos_crudos.copy()\n",
    "datos_limpios['producto'] = datos_limpios['producto'].fillna('Desconocido').str.capitalize()\n",
    "datos_limpios['precio'] = pd.to_numeric(datos_limpios['precio'], errors='coerce').fillna(0)\n",
    "datos_limpios['fecha'] = pd.to_datetime(datos_limpios['fecha'], format='mixed', dayfirst=False)\n",
    "print(datos_limpios)\n",
    "print(f\"   Tipos: {dict(datos_limpios.dtypes)}\")\n",
    "\n",
    "# 3. CONSUMO: An√°lisis r√°pido\n",
    "print(\"\\n3Ô∏è‚É£ CONSUMO - An√°lisis listo para dashboards:\")\n",
    "print(f\"   Total productos: {len(datos_limpios)}\")\n",
    "print(f\"   Precio promedio: ${datos_limpios['precio'].mean():.2f}\")\n",
    "print(f\"   Rango de fechas: {datos_limpios['fecha'].min().date()} a {datos_limpios['fecha'].max().date()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Datos pasaron de crudos a listos para an√°lisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adf828",
   "metadata": {},
   "source": [
    "## 1. ETL vs ELT\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "**ETL (Extract, Transform, Load)** y **ELT (Extract, Load, Transform)** son los dos paradigmas principales para mover datos desde sus fuentes hasta un destino anal√≠tico.\n",
    "\n",
    "**ETL** ‚Äî Paradigma tradicional:\n",
    "> `[Fuentes] ‚Üí EXTRACT ‚Üí TRANSFORM ‚Üí LOAD ‚Üí [Data Warehouse]`\n",
    "\n",
    "Transforma los datos **antes** de cargarlos. Los datos llegan ya procesados al destino.\n",
    "\n",
    "**ELT** ‚Äî Paradigma moderno (cloud):\n",
    "> `[Fuentes] ‚Üí EXTRACT ‚Üí LOAD ‚Üí TRANSFORM ‚Üí [Consultas]`\n",
    "\n",
    "Carga los datos en bruto y los transforma **dentro** del warehouse aprovechando su potencia.\n",
    "\n",
    "| Aspecto | ETL | ELT |\n",
    "|---|---|---|\n",
    "| Transformaci√≥n | Antes de cargar | Despu√©s de cargar |\n",
    "| Velocidad | M√°s lento | M√°s r√°pido |\n",
    "| Datos crudos | No disponibles | Disponibles |\n",
    "| Flexibilidad | Menor | Mayor |\n",
    "| Infraestructura | Servidor propio | Cloud warehouse |\n",
    "| Privacidad | Mejor (transforma antes) | Requiere gobernanza |\n",
    "\n",
    "**¬øCu√°l usar?**\n",
    "\n",
    "- **ETL** cuando hay transformaciones complejas, preocupaciones de privacidad o el warehouse tiene recursos limitados\n",
    "- **ELT** cuando usas plataformas cloud (Snowflake, BigQuery) y necesitas flexibilidad\n",
    "- **Tendencia actual:** ELT gana popularidad. Herramientas modernas: dbt, Dataform, Matillion\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Conocer ETL y ELT sirve para:\n",
    "\n",
    "- **Elegir la arquitectura correcta** seg√∫n los recursos disponibles y requisitos del proyecto\n",
    "- **Dise√±ar flujos de datos** que lleven informaci√≥n desde m√∫ltiples fuentes hasta un repositorio centralizado\n",
    "- **Limpiar y transformar datos** de forma sistem√°tica antes o despu√©s de almacenarlos\n",
    "- **Comprender las tendencias actuales** del ecosistema de datos\n",
    "- **Implementar pipelines reproducibles** que garanticen la calidad y consistencia de los datos\n",
    "- **Aprovechar plataformas cloud** modernas con el enfoque ELT\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, implementamos un proceso ETL completo en Python: extracci√≥n de datos de ventas, transformaci√≥n con limpieza y columnas calculadas, y carga al destino final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d06eec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    EJEMPLO: ETL SIMPLE EN PYTHON                     \n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ EXTRACT - Extrayendo datos...\n",
      "‚úì Extra√≠dos 5 registros de ventas\n",
      "   id producto  cantidad  precio       fecha\n",
      "0   1   laptop       2.0    1000  2024-01-15\n",
      "1   2    mouse       5.0      25  2024-01-16\n",
      "2   3  TECLADO       NaN      50  2024-01-16\n",
      "3   4  Monitor       1.0     300         NaN\n",
      "4   5   laptop       3.0    1000  2024-01-17\n",
      "\n",
      "2Ô∏è‚É£ TRANSFORM - Transformando y limpiando datos...\n",
      "‚úì Datos transformados:\n",
      "   id producto  cantidad  precio      fecha   total     a√±o  mes\n",
      "0   1   laptop       2.0    1000 2024-01-15  2000.0  2024.0  1.0\n",
      "1   2    mouse       5.0      25 2024-01-16   125.0  2024.0  1.0\n",
      "2   3  teclado       NaN      50 2024-01-16     NaN  2024.0  1.0\n",
      "3   4  monitor       1.0     300        NaT   300.0     NaN  NaN\n",
      "4   5   laptop       3.0    1000 2024-01-17  3000.0  2024.0  1.0\n",
      "\n",
      "3Ô∏è‚É£ LOAD - Cargando datos procesados...\n",
      "‚úì Datos cargados en ventas_procesadas.csv\n",
      "\n",
      "======================================================================\n",
      "                       RESUMEN DEL PROCESO ETL                        \n",
      "======================================================================\n",
      "\n",
      "Registros extra√≠dos:    5\n",
      "Registros transformados: 5\n",
      "Registros cargados:     5\n",
      "Columnas originales:    5\n",
      "Columnas finales:       8\n",
      "Valores nulos corregidos: 2\n",
      "\n",
      "‚úì Archivo temporal ventas_procesadas.csv eliminado\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO: ETL SIMPLE EN PYTHON\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. EXTRACT: Simular extracci√≥n de datos de m√∫ltiples fuentes\n",
    "print(\"\\n1Ô∏è‚É£ EXTRACT - Extrayendo datos...\")\n",
    "\n",
    "ventas_raw = {\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'producto': ['laptop', 'mouse', 'TECLADO', 'Monitor', 'laptop'],\n",
    "    'cantidad': [2, 5, None, 1, 3],\n",
    "    'precio': [1000, 25, 50, 300, 1000],\n",
    "    'fecha': ['2024-01-15', '2024-01-16', '2024-01-16', None, '2024-01-17']\n",
    "}\n",
    "\n",
    "df_ventas = pd.DataFrame(ventas_raw)\n",
    "print(f\"‚úì Extra√≠dos {len(df_ventas)} registros de ventas\")\n",
    "print(df_ventas)\n",
    "\n",
    "# 2. TRANSFORM: Limpiar y transformar\n",
    "print(\"\\n2Ô∏è‚É£ TRANSFORM - Transformando y limpiando datos...\")\n",
    "\n",
    "df_transformado = df_ventas.copy()\n",
    "\n",
    "# Normalizar nombres de productos\n",
    "df_transformado['producto'] = df_transformado['producto'].str.lower().str.strip()\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "df_transformado['cantidad'].fillna(1, inplace=True)\n",
    "df_transformado['fecha'].fillna('2024-01-15', inplace=True)\n",
    "\n",
    "# Crear columnas calculadas\n",
    "df_transformado['total'] = df_transformado['cantidad'] * df_transformado['precio']\n",
    "df_transformado['fecha'] = pd.to_datetime(df_transformado['fecha'])\n",
    "\n",
    "# Agregar informaci√≥n adicional\n",
    "df_transformado['a√±o'] = df_transformado['fecha'].dt.year\n",
    "df_transformado['mes'] = df_transformado['fecha'].dt.month\n",
    "\n",
    "print(\"‚úì Datos transformados:\")\n",
    "print(df_transformado)\n",
    "\n",
    "# 3. LOAD: Cargar a destino\n",
    "print(\"\\n3Ô∏è‚É£ LOAD - Cargando datos procesados...\")\n",
    "\n",
    "# En producci√≥n: cargar√≠as a base de datos\n",
    "# df_transformado.to_sql('ventas', con=engine, if_exists='append')\n",
    "\n",
    "# Para ejemplo: guardar a CSV\n",
    "output_file = 'ventas_procesadas.csv'\n",
    "df_transformado.to_csv(output_file, index=False)\n",
    "print(f\"‚úì Datos cargados en {output_file}\")\n",
    "\n",
    "# Resumen del proceso\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESUMEN DEL PROCESO ETL\".center(70))\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "Registros extra√≠dos:    {len(df_ventas)}\n",
    "Registros transformados: {len(df_transformado)}\n",
    "Registros cargados:     {len(df_transformado)}\n",
    "Columnas originales:    {len(df_ventas.columns)}\n",
    "Columnas finales:       {len(df_transformado.columns)}\n",
    "Valores nulos corregidos: {df_ventas.isna().sum().sum()}\n",
    "\"\"\")\n",
    "\n",
    "# Limpiar archivo de ejemplo\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"‚úì Archivo temporal {output_file} eliminado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9889a99",
   "metadata": {},
   "source": [
    "## 2. Pipelines de Datos\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "Un **pipeline de datos** es una secuencia automatizada de pasos que mueve y transforma datos desde un origen hasta un destino. Funciona como una cadena de montaje digital: cada etapa recibe datos, los procesa y los pasa a la siguiente.\n",
    "\n",
    "**Componentes t√≠picos:**\n",
    "\n",
    "1. **Source** ‚Üí Base de datos, API, archivos, sensores/IoT\n",
    "2. **Ingestion** ‚Üí Conectores, schedulers, validaci√≥n inicial\n",
    "3. **Processing** ‚Üí Transformaciones, limpieza, agregaciones\n",
    "4. **Storage** ‚Üí Data Lake, Data Warehouse, cach√©\n",
    "5. **Serving** ‚Üí APIs, dashboards, reportes, modelos ML\n",
    "\n",
    "**Tipos de pipelines:**\n",
    "\n",
    "| Tipo | Descripci√≥n | Ejemplo |\n",
    "|---|---|---|\n",
    "| **Batch** | Procesa en intervalos (horario, diario) | Reporte diario de ventas |\n",
    "| **Streaming** | Procesa continuamente, baja latencia | Detecci√≥n de fraude en tiempo real |\n",
    "| **H√≠brido** | Combina batch y streaming | Alertas streaming + an√°lisis hist√≥rico batch |\n",
    "\n",
    "**Herramientas comunes:** Scripts Python (simple), Apache Airflow (est√°ndar industria), Prefect/Dagster (modernas), Cloud (AWS Glue, GCP Dataflow, Azure Data Factory), Streaming (Kafka, Spark Streaming).\n",
    "\n",
    "**Mejores pr√°cticas:** Idempotencia, manejo de errores con retries, logging completo, testing con datos de prueba, monitoreo con alertas, documentaci√≥n y versionado.\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Construir pipelines de datos sirve para:\n",
    "\n",
    "- **Automatizar flujos repetitivos** eliminando la intervenci√≥n manual en procesos de datos\n",
    "- **Conectar m√∫ltiples sistemas** desde fuentes de origen hasta almacenamiento y herramientas de BI\n",
    "- **Elegir entre batch y streaming** seg√∫n las necesidades de latencia del negocio\n",
    "- **Escalar el procesamiento** conforme crecen los vol√∫menes de datos\n",
    "- **Monitorear y alertar** cuando algo falla en el procesamiento de datos\n",
    "- **Garantizar reproducibilidad** donde los mismos inputs siempre producen los mismos outputs\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, construimos un pipeline con una clase Python que implementa los pasos de extracci√≥n, validaci√≥n, transformaci√≥n y carga, con logging de eventos y manejo de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567ad508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                  EJEMPLO: PIPELINE DE PROCESAMIENTO                  \n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "          üöÄ Iniciando Pipeline: Procesamiento Ventas Diario           \n",
      "======================================================================\n",
      "\n",
      "üìù [2026-02-21 22:22:53] EXTRACT: Extrayendo datos de base_datos_ventas...\n",
      "üìù [2026-02-21 22:22:53] ‚úì Extra√≠dos 100 registros\n",
      "üìù [2026-02-21 22:22:53] VALIDATE: Validando calidad de datos...\n",
      "üìù [2026-02-21 22:22:54] ‚úì Validaci√≥n exitosa, datos limpios\n",
      "üìù [2026-02-21 22:22:54] TRANSFORM: Aplicando transformaciones...\n",
      "üìù [2026-02-21 22:22:54] ‚úì Transformaciones aplicadas\n",
      "üìù [2026-02-21 22:22:54] LOAD: Cargando datos a warehouse_ventas_procesadas...\n",
      "üìù [2026-02-21 22:22:54] ‚úì 100 registros cargados exitosamente\n",
      "\n",
      "======================================================================\n",
      "                  ‚úÖ PIPELINE COMPLETADO EXITOSAMENTE                  \n",
      "======================================================================\n",
      "\n",
      "üìä RESUMEN POR CATEGOR√çA:\n",
      "          valor                valor_normalizado\n",
      "          count    mean    sum              mean\n",
      "categoria                                       \n",
      "A            37  519.03  19204              0.08\n",
      "B            31  561.68  17412              0.21\n",
      "C            32  405.22  12967             -0.29\n",
      "\n",
      "üìã Total de eventos registrados: 8\n",
      "\n",
      "üí° Este es un pipeline simple. En producci√≥n:\n",
      "  ‚Ä¢ Usar√≠as Airflow para orquestaci√≥n\n",
      "  ‚Ä¢ Tendr√≠as m√∫ltiples pipelines interconectados\n",
      "  ‚Ä¢ Monitorear√≠as con herramientas como Grafana\n",
      "  ‚Ä¢ Usar√≠as colas de mensajes (Kafka, RabbitMQ)\n",
      "  ‚Ä¢ Implementar√≠as reintentos autom√°ticos\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO: PIPELINE DE PROCESAMIENTO\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"Pipeline simple de procesamiento de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.logs = []\n",
    "        \n",
    "    def log(self, message):\n",
    "        \"\"\"Registrar eventos del pipeline\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        self.logs.append(log_entry)\n",
    "        print(f\"üìù {log_entry}\")\n",
    "    \n",
    "    def extract(self, source):\n",
    "        \"\"\"Paso 1: Extraer datos\"\"\"\n",
    "        self.log(f\"EXTRACT: Extrayendo datos de {source}...\")\n",
    "        time.sleep(0.5)  # Simular tiempo de extracci√≥n\n",
    "        \n",
    "        # Simular datos extra√≠dos\n",
    "        data = pd.DataFrame({\n",
    "            'id': range(1, 101),\n",
    "            'valor': np.random.randint(10, 1000, 100),\n",
    "            'categoria': np.random.choice(['A', 'B', 'C'], 100),\n",
    "            'fecha': pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "        })\n",
    "        \n",
    "        self.log(f\"‚úì Extra√≠dos {len(data)} registros\")\n",
    "        return data\n",
    "    \n",
    "    def validate(self, data):\n",
    "        \"\"\"Paso 2: Validar datos\"\"\"\n",
    "        self.log(\"VALIDATE: Validando calidad de datos...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Validaciones\n",
    "        checks = {\n",
    "            'No hay datos': len(data) == 0,\n",
    "            'Valores nulos': data.isna().any().any(),\n",
    "            'Duplicados': data.duplicated().any()\n",
    "        }\n",
    "        \n",
    "        issues = [check for check, failed in checks.items() if failed]\n",
    "        \n",
    "        if issues:\n",
    "            self.log(f\"‚ö†Ô∏è Problemas encontrados: {', '.join(issues)}\")\n",
    "        else:\n",
    "            self.log(\"‚úì Validaci√≥n exitosa, datos limpios\")\n",
    "            \n",
    "        return data, issues\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Paso 3: Transformar datos\"\"\"\n",
    "        self.log(\"TRANSFORM: Aplicando transformaciones...\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Agregar columnas calculadas\n",
    "        data['valor_normalizado'] = (data['valor'] - data['valor'].mean()) / data['valor'].std()\n",
    "        \n",
    "        # Agregar informaci√≥n temporal\n",
    "        data['dia_semana'] = data['fecha'].dt.day_name()\n",
    "        \n",
    "        # Agrupar y agregar\n",
    "        resumen = data.groupby('categoria').agg({\n",
    "            'valor': ['count', 'mean', 'sum'],\n",
    "            'valor_normalizado': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        self.log(f\"‚úì Transformaciones aplicadas\")\n",
    "        return data, resumen\n",
    "    \n",
    "    def load(self, data, destination):\n",
    "        \"\"\"Paso 4: Cargar datos\"\"\"\n",
    "        self.log(f\"LOAD: Cargando datos a {destination}...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # En producci√≥n: cargar a base de datos\n",
    "        # data.to_sql(destination, con=engine, if_exists='replace')\n",
    "        \n",
    "        self.log(f\"‚úì {len(data)} registros cargados exitosamente\")\n",
    "        return True\n",
    "    \n",
    "    def run(self, source, destination):\n",
    "        \"\"\"Ejecutar pipeline completo\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üöÄ Iniciando Pipeline: {self.name}\".center(70))\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Extract\n",
    "            data = self.extract(source)\n",
    "            \n",
    "            # 2. Validate\n",
    "            data, issues = self.validate(data)\n",
    "            \n",
    "            # 3. Transform\n",
    "            data_transformed, resumen = self.transform(data)\n",
    "            \n",
    "            # 4. Load\n",
    "            success = self.load(data_transformed, destination)\n",
    "            \n",
    "            # Resumen\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"‚úÖ PIPELINE COMPLETADO EXITOSAMENTE\".center(70))\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            print(\"üìä RESUMEN POR CATEGOR√çA:\")\n",
    "            print(resumen)\n",
    "            \n",
    "            return data_transformed\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"‚ùå ERROR: {str(e)}\")\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"‚ùå PIPELINE FALL√ì\".center(70))\n",
    "            print(f\"{'='*70}\")\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            print(f\"\\nüìã Total de eventos registrados: {len(self.logs)}\")\n",
    "\n",
    "# Ejecutar pipeline\n",
    "pipeline = DataPipeline(\"Procesamiento Ventas Diario\")\n",
    "datos_finales = pipeline.run(\n",
    "    source=\"base_datos_ventas\",\n",
    "    destination=\"warehouse_ventas_procesadas\"\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Este es un pipeline simple. En producci√≥n:\")\n",
    "print(\"  ‚Ä¢ Usar√≠as Airflow para orquestaci√≥n\")\n",
    "print(\"  ‚Ä¢ Tendr√≠as m√∫ltiples pipelines interconectados\")\n",
    "print(\"  ‚Ä¢ Monitorear√≠as con herramientas como Grafana\")\n",
    "print(\"  ‚Ä¢ Usar√≠as colas de mensajes (Kafka, RabbitMQ)\")\n",
    "print(\"  ‚Ä¢ Implementar√≠as reintentos autom√°ticos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ce242",
   "metadata": {},
   "source": [
    "## 3. Formatos de Datos\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "Los **formatos de datos** son las estructuras y codificaciones que definen c√≥mo se almacenan, organizan y transmiten los datos en archivos. Los m√°s comunes en ingenier√≠a de datos son CSV (texto plano tabular), JSON (pares clave-valor), Parquet (binario columnar) y Avro (binario orientado a filas), cada uno con caracter√≠sticas distintas de velocidad, tama√±o y compatibilidad.\n",
    "\n",
    "**Comparaci√≥n de formatos:**\n",
    "\n",
    "| Formato | Tipo | Legible | Compresi√≥n | Velocidad | Uso principal |\n",
    "|---|---|---|---|---|---|\n",
    "| **CSV** | Texto plano | S√≠ | No | Lenta | Intercambio, exploraci√≥n |\n",
    "| **JSON** | Texto clave-valor | S√≠ | No | Lenta | APIs, datos anidados |\n",
    "| **Parquet** | Binario columnar | No | Excelente (90%‚Üì) | Muy r√°pida | Analytics, Data Lake |\n",
    "| **Avro** | Binario filas | No | Buena | R√°pida escritura | Streaming, Kafka |\n",
    "\n",
    "**Detalles por formato:**\n",
    "\n",
    "- **CSV**: Universal y editable manualmente. Sin tipos de datos (todo es texto), sin compresi√≥n nativa. Problemas con separadores dentro de los datos. Ideal para compatibilidad m√°xima.\n",
    "- **JSON**: Estructura flexible con datos anidados. Est√°ndar para APIs web y configuraciones. Soporta tipos b√°sicos pero puede ser verboso para grandes vol√∫menes.\n",
    "- **Parquet**: Lee solo las columnas necesarias (column pruning). Preserva esquema y tipos. Excelente compresi√≥n (hasta 90% menos que CSV). Soporta particionamiento. Compatible con Spark, Hive, etc.\n",
    "- **Avro**: Esquema auto-contenido que evoluciona sin romper compatibilidad. Optimizado para escritura r√°pida y streaming (Kafka). Menos com√∫n que Parquet en analytics.\n",
    "\n",
    "**Regla general de uso:**\n",
    "\n",
    "- **Desarrollo/Exploraci√≥n** ‚Üí CSV, JSON (f√°cil de inspeccionar)\n",
    "- **Producci√≥n/Analytics** ‚Üí Parquet (eficiencia y velocidad)\n",
    "- **APIs y microservicios** ‚Üí JSON (est√°ndar web)\n",
    "- **Streaming** ‚Üí Avro o Parquet\n",
    "- **Intercambio con externos** ‚Üí CSV (compatibilidad universal)\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Conocer los formatos de datos sirve para:\n",
    "\n",
    "- **Elegir el formato √≥ptimo** seg√∫n el caso de uso: exploraci√≥n, producci√≥n, intercambio o streaming\n",
    "- **Reducir costos de almacenamiento** usando formatos comprimidos como Parquet (hasta 90% menos que CSV)\n",
    "- **Acelerar la lectura y escritura** seleccionando formatos binarios para grandes vol√∫menes\n",
    "- **Preservar tipos de datos y esquemas** evitando p√©rdida de informaci√≥n en conversiones\n",
    "- **Mejorar el rendimiento de queries** con formatos columnares que leen solo las columnas necesarias\n",
    "- **Facilitar la interoperabilidad** entre diferentes sistemas, plataformas y herramientas\n",
    "\n",
    "La elecci√≥n del formato correcto impacta directamente en la eficiencia de todo el pipeline de datos.\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, crearemos un dataset de ejemplo y lo guardaremos en CSV, JSON y Parquet, comparando tiempos de escritura, lectura, tama√±o de archivo y rendimiento en lectura selectiva de columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd3c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                   COMPARACI√ìN PR√ÅCTICA DE FORMATOS                   \n",
      "======================================================================\n",
      "\n",
      "üìä Dataset de ejemplo: 10000 filas, 6 columnas\n",
      "   id      nombre    categoria  precio  cantidad      fecha\n",
      "0   1  Producto_1     Deportes  399.70        28 2023-01-01\n",
      "1   2  Producto_2         Ropa  478.70        22 2023-01-02\n",
      "2   3  Producto_3  Electr√≥nica  856.00        16 2023-01-03\n",
      "3   4  Producto_4     Deportes  346.60        13 2023-01-04\n",
      "4   5  Producto_5     Deportes  870.95        62 2023-01-05\n",
      "\n",
      "1Ô∏è‚É£ Guardando como CSV...\n",
      "   ‚úì Tiempo: 0.033s | Tama√±o: 0.47 MB\n",
      "\n",
      "2Ô∏è‚É£ Guardando como JSON...\n",
      "   ‚úì Tiempo: 0.022s | Tama√±o: 1.18 MB\n",
      "\n",
      "3Ô∏è‚É£ Guardando como PARQUET...\n",
      "   ‚úì Tiempo: 0.118s | Tama√±o: 0.28 MB\n",
      "\n",
      "======================================================================\n",
      "                     üìä COMPARACI√ìN DE RESULTADOS                      \n",
      "======================================================================\n",
      "Formato  Tama√±o (MB)  Tiempo Escritura (s)   % vs CSV\n",
      "    CSV     0.465096              0.032845 100.000000\n",
      "   JSON     1.180449              0.022075 253.807762\n",
      "Parquet     0.279115              0.117951  60.012344\n",
      "\n",
      "üí° Parquet es 1.7x m√°s peque√±o que CSV\n",
      "\n",
      "======================================================================\n",
      "                        ‚ö° VELOCIDAD DE LECTURA                        \n",
      "======================================================================\n",
      "CSV:     0.042s\n",
      "JSON:    0.061s\n",
      "Parquet: 0.135s\n",
      "\n",
      "üí° Parquet es 0.3x m√°s r√°pido que CSV en lectura\n",
      "\n",
      "======================================================================\n",
      "                   üìñ LECTURA SELECTIVA DE COLUMNAS                    \n",
      "======================================================================\n",
      "CSV (lectura selectiva):     0.010s\n",
      "Parquet (lectura selectiva): 0.005s\n",
      "\n",
      "üí° Para lectura selectiva, Parquet es 2.0x m√°s eficiente\n",
      "\n",
      "‚úÖ Archivos temporales eliminados\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARACI√ìN PR√ÅCTICA DE FORMATOS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Crear dataset de ejemplo\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, 10001),\n",
    "    'nombre': [f'Producto_{i}' for i in range(1, 10001)],\n",
    "    'categoria': np.random.choice(['Electr√≥nica', 'Hogar', 'Deportes', 'Ropa'], 10000),\n",
    "    'precio': np.round(np.random.uniform(10, 1000, 10000), 2),\n",
    "    'cantidad': np.random.randint(1, 100, 10000),\n",
    "    'fecha': pd.date_range('2023-01-01', periods=10000, freq='D')\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Dataset de ejemplo: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "print(df.head())\n",
    "\n",
    "# Funci√≥n para obtener tama√±o de archivo\n",
    "def get_file_size(filename):\n",
    "    if os.path.exists(filename):\n",
    "        size_bytes = os.path.getsize(filename)\n",
    "        size_mb = size_bytes / (1024 * 1024)\n",
    "        return size_bytes, size_mb\n",
    "    return 0, 0\n",
    "\n",
    "# 1. Guardar como CSV\n",
    "print(\"\\n1Ô∏è‚É£ Guardando como CSV...\")\n",
    "csv_file = 'datos_ejemplo.csv'\n",
    "start_time = time.time()\n",
    "df.to_csv(csv_file, index=False)\n",
    "csv_time = time.time() - start_time\n",
    "csv_bytes, csv_mb = get_file_size(csv_file)\n",
    "print(f\"   ‚úì Tiempo: {csv_time:.3f}s | Tama√±o: {csv_mb:.2f} MB\")\n",
    "\n",
    "# 2. Guardar como JSON\n",
    "print(\"\\n2Ô∏è‚É£ Guardando como JSON...\")\n",
    "json_file = 'datos_ejemplo.json'\n",
    "start_time = time.time()\n",
    "df.to_json(json_file, orient='records', date_format='iso')\n",
    "json_time = time.time() - start_time\n",
    "json_bytes, json_mb = get_file_size(json_file)\n",
    "print(f\"   ‚úì Tiempo: {json_time:.3f}s | Tama√±o: {json_mb:.2f} MB\")\n",
    "\n",
    "# 3. Guardar como Parquet\n",
    "print(\"\\n3Ô∏è‚É£ Guardando como PARQUET...\")\n",
    "parquet_file = 'datos_ejemplo.parquet'\n",
    "start_time = time.time()\n",
    "df.to_parquet(parquet_file, engine='pyarrow', compression='snappy')\n",
    "parquet_time = time.time() - start_time\n",
    "parquet_bytes, parquet_mb = get_file_size(parquet_file)\n",
    "print(f\"   ‚úì Tiempo: {parquet_time:.3f}s | Tama√±o: {parquet_mb:.2f} MB\")\n",
    "\n",
    "# Comparaci√≥n de resultados\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä COMPARACI√ìN DE RESULTADOS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "resultados = pd.DataFrame({\n",
    "    'Formato': ['CSV', 'JSON', 'Parquet'],\n",
    "    'Tama√±o (MB)': [csv_mb, json_mb, parquet_mb],\n",
    "    'Tiempo Escritura (s)': [csv_time, json_time, parquet_time],\n",
    "    '% vs CSV': [100, (json_mb/csv_mb)*100, (parquet_mb/csv_mb)*100]\n",
    "})\n",
    "\n",
    "print(resultados.to_string(index=False))\n",
    "print(f\"\\nüí° Parquet es {csv_mb/parquet_mb:.1f}x m√°s peque√±o que CSV\")\n",
    "\n",
    "# Velocidad de lectura\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚ö° VELOCIDAD DE LECTURA\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv(csv_file)\n",
    "csv_read_time = time.time() - start_time\n",
    "print(f\"CSV:     {csv_read_time:.3f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_json = pd.read_json(json_file)\n",
    "json_read_time = time.time() - start_time\n",
    "print(f\"JSON:    {json_read_time:.3f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_parquet = pd.read_parquet(parquet_file)\n",
    "parquet_read_time = time.time() - start_time\n",
    "print(f\"Parquet: {parquet_read_time:.3f}s\")\n",
    "\n",
    "print(f\"\\nüí° Parquet es {csv_read_time/parquet_read_time:.1f}x m√°s r√°pido que CSV en lectura\")\n",
    "\n",
    "# Lectura selectiva de columnas (ventaja clave de Parquet)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìñ LECTURA SELECTIVA DE COLUMNAS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "columnas_necesarias = ['id', 'categoria', 'precio']\n",
    "\n",
    "start_time = time.time()\n",
    "df_csv_cols = pd.read_csv(csv_file, usecols=columnas_necesarias)\n",
    "csv_selective_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "df_parquet_cols = pd.read_parquet(parquet_file, columns=columnas_necesarias)\n",
    "parquet_selective_time = time.time() - start_time\n",
    "\n",
    "print(f\"CSV (lectura selectiva):     {csv_selective_time:.3f}s\")\n",
    "print(f\"Parquet (lectura selectiva): {parquet_selective_time:.3f}s\")\n",
    "print(f\"\\nüí° Para lectura selectiva, Parquet es {csv_selective_time/parquet_selective_time:.1f}x m√°s eficiente\")\n",
    "\n",
    "# Limpiar archivos temporales\n",
    "for file in [csv_file, json_file, parquet_file]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"\\n‚úÖ Archivos temporales eliminados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334371",
   "metadata": {},
   "source": [
    "## 4. APIs y Web Scraping\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "Las **APIs (Application Programming Interfaces)** son interfaces estructuradas y oficiales que permiten a las aplicaciones comunicarse entre s√≠ para intercambiar datos. El **web scraping** es la t√©cnica de extraer datos directamente del HTML de p√°ginas web cuando no existe una API disponible. Ambos son m√©todos esenciales para obtener datos de la web.\n",
    "\n",
    "**Tipos de APIs:**\n",
    "\n",
    "| Tipo | Descripci√≥n | Formato | Uso actual |\n",
    "|---|---|---|---|\n",
    "| **REST** | HTTP est√°ndar (GET, POST, PUT, DELETE) | JSON | M√°s com√∫n en la web |\n",
    "| **GraphQL** | Solicitas exactamente los datos necesarios | JSON | Moderno y eficiente |\n",
    "| **SOAP** | Protocolo formal basado en XML | XML | Legacy, menos com√∫n hoy |\n",
    "\n",
    "**Conceptos clave de APIs:**\n",
    "\n",
    "- **Endpoint**: URL espec√≠fica de la API (ej: `api.ejemplo.com/users`)\n",
    "- **Authentication**: API key, OAuth, tokens de acceso\n",
    "- **Rate limiting**: L√≠mite de requests por tiempo (ej: 100 req/minuto)\n",
    "- **Pagination**: Dividir resultados grandes en p√°ginas\n",
    "- **Status codes**: 200 OK, 401 Unauthorized, 404 Not Found, 429 Too Many Requests\n",
    "\n",
    "**Herramientas Python para Web Scraping:**\n",
    "\n",
    "- **requests**: Hacer peticiones HTTP y obtener contenido HTML\n",
    "- **BeautifulSoup**: Parsear HTML/XML, navegar estructura DOM\n",
    "- **Selenium**: Automatizar navegador real, JavaScript din√°mico\n",
    "- **Scrapy**: Framework completo para spiders y crawlers\n",
    "\n",
    "**¬øAPI o Web Scraping?**\n",
    "\n",
    "- **Usa API cuando**: est√° disponible, necesitas datos estructurados, es un proyecto en producci√≥n\n",
    "- **Usa Scraping cuando**: no hay API, datos son p√∫blicos, es investigaci√≥n puntual y legal\n",
    "\n",
    "**Consideraciones legales del scraping:**\n",
    "\n",
    "- Revisar t√©rminos de servicio del sitio y respetar `robots.txt`\n",
    "- No sobrecargar servidores (usar rate limiting con delays)\n",
    "- Evitar scraping de datos personales sensibles o datos detr√°s de login\n",
    "- Dar atribuci√≥n cuando se usen los datos obtenidos\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Dominar APIs y web scraping sirve para:\n",
    "\n",
    "- **Obtener datos de fuentes externas** como redes sociales, servicios financieros, APIs gubernamentales y sitios web\n",
    "- **Automatizar la recolecci√≥n** de datos que se actualizan frecuentemente\n",
    "- **Integrar m√∫ltiples fuentes** en un mismo pipeline de an√°lisis\n",
    "- **Construir datasets enriquecidos** combinando datos internos con informaci√≥n externa\n",
    "- **Implementar manejo robusto de errores** con reintentos, timeouts y rate limiting\n",
    "- **Conocer las consideraciones legales y √©ticas** del scraping para actuar responsablemente\n",
    "\n",
    "En an√°lisis de datos, saber consumir APIs es una habilidad fundamental para acceder a datos actualizados y estructurados.\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, consumiremos APIs p√∫blicas con la librer√≠a `requests`, manejaremos par√°metros de consulta e implementaremos funciones robustas con reintentos y manejo de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e113fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                   EJEMPLO 1: CONSUMIR API P√öBLICA                    \n",
      "======================================================================\n",
      "\n",
      "üì° Consumiendo API p√∫blica: JSONPlaceholder\n",
      "   (API de prueba con datos ficticios)\n",
      "\n",
      " Status code: 200 ‚úÖ (OK)\n",
      "‚úì Obtenidos 10 usuarios\n",
      "\n",
      "üìä Datos obtenidos:\n",
      "   id              name                      email  \\\n",
      "0   1     Leanne Graham          Sincere@april.biz   \n",
      "1   2      Ervin Howell          Shanna@melissa.tv   \n",
      "2   3  Clementine Bauch         Nathan@yesenia.net   \n",
      "3   4  Patricia Lebsack  Julianne.OConner@kory.org   \n",
      "4   5  Chelsey Dietrich   Lucio_Hettinger@annie.ca   \n",
      "\n",
      "                                             company  \n",
      "0  {'name': 'Romaguera-Crona', 'catchPhrase': 'Mu...  \n",
      "1  {'name': 'Deckow-Crist', 'catchPhrase': 'Proac...  \n",
      "2  {'name': 'Romaguera-Jacobson', 'catchPhrase': ...  \n",
      "3  {'name': 'Robel-Corkery', 'catchPhrase': 'Mult...  \n",
      "4  {'name': 'Keebler LLC', 'catchPhrase': 'User-c...  \n",
      "\n",
      "üìà An√°lisis r√°pido:\n",
      "  ‚Ä¢ Total usuarios: 10\n",
      "  ‚Ä¢ Campos disponibles: id, name, username, email, address, phone, website, company\n",
      "\n",
      "üè¢ Empresas √∫nicas: 10\n",
      "company_name\n",
      "Romaguera-Crona       1\n",
      "Deckow-Crist          1\n",
      "Romaguera-Jacobson    1\n",
      "Robel-Corkery         1\n",
      "Keebler LLC           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "                    EJEMPLO 2: API CON PAR√ÅMETROS                     \n",
      "======================================================================\n",
      "\n",
      "üîç Obteniendo posts de un usuario espec√≠fico\n",
      "‚úì Obtenidos 5 posts\n",
      "\n",
      "üìù T√≠tulos de posts:\n",
      "  1. sunt aut facere repellat provident occaecati excep...\n",
      "  2. qui est esse...\n",
      "  3. ea molestias quasi exercitationem repellat qui ips...\n",
      "  4. eum et est occaecati...\n",
      "  5. nesciunt quas odio...\n",
      "\n",
      "======================================================================\n",
      "                 EJEMPLO 3: FUNCI√ìN ROBUSTA PARA APIs                 \n",
      "======================================================================\n",
      "\n",
      "üß™ Probando funci√≥n robusta:\n",
      "‚úÖ Datos obtenidos: Leanne Graham\n",
      "\n",
      "‚úÖ Ejemplos de APIs completados\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO 1: CONSUMIR API P√öBLICA\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# API p√∫blica gratuita (no requiere key)\n",
    "print(\"\\nüì° Consumiendo API p√∫blica: JSONPlaceholder\")\n",
    "print(\"   (API de prueba con datos ficticios)\")\n",
    "\n",
    "try:\n",
    "    url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    \n",
    "    print(f\"\\n Status code: {response.status_code}\", end=\"\")\n",
    "    if response.status_code == 200:\n",
    "        print(\" ‚úÖ (OK)\")\n",
    "    else:\n",
    "        print(f\" ‚ùå (Error)\")\n",
    "    \n",
    "    usuarios = response.json()\n",
    "    print(f\"‚úì Obtenidos {len(usuarios)} usuarios\")\n",
    "    \n",
    "    df_usuarios = pd.DataFrame(usuarios)\n",
    "    \n",
    "    print(\"\\nüìä Datos obtenidos:\")\n",
    "    print(df_usuarios[['id', 'name', 'email', 'company']].head())\n",
    "    \n",
    "    print(\"\\nüìà An√°lisis r√°pido:\")\n",
    "    print(f\"  ‚Ä¢ Total usuarios: {len(df_usuarios)}\")\n",
    "    print(f\"  ‚Ä¢ Campos disponibles: {', '.join(df_usuarios.columns)}\")\n",
    "    \n",
    "    df_usuarios['company_name'] = df_usuarios['company'].apply(lambda x: x['name'])\n",
    "    print(f\"\\nüè¢ Empresas √∫nicas: {df_usuarios['company_name'].nunique()}\")\n",
    "    print(df_usuarios['company_name'].value_counts().head())\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error al conectar con API: {e}\")\n",
    "\n",
    "# EJEMPLO 2: API con par√°metros de consulta\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EJEMPLO 2: API CON PAR√ÅMETROS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüîç Obteniendo posts de un usuario espec√≠fico\")\n",
    "\n",
    "try:\n",
    "    url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "    params = {\n",
    "        'userId': 1,\n",
    "        '_limit': 5\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        posts = response.json()\n",
    "        df_posts = pd.DataFrame(posts)\n",
    "        \n",
    "        print(f\"‚úì Obtenidos {len(df_posts)} posts\")\n",
    "        print(\"\\nüìù T√≠tulos de posts:\")\n",
    "        for idx, post in df_posts.iterrows():\n",
    "            print(f\"  {idx+1}. {post['title'][:50]}...\")\n",
    "            \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# EJEMPLO 3: Funci√≥n robusta para APIs con reintentos\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EJEMPLO 3: FUNCI√ìN ROBUSTA PARA APIs\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def obtener_datos_api(url, max_retries=3):\n",
    "    \"\"\"Funci√≥n robusta para consumir APIs con reintentos\"\"\"\n",
    "    \n",
    "    for intento in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"‚ö†Ô∏è Rate limit alcanzado, esperando...\")\n",
    "                time.sleep(5)\n",
    "            elif response.status_code == 404:\n",
    "                print(f\"‚ùå Recurso no encontrado: {url}\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Status code: {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"‚è±Ô∏è Timeout en intento {intento+1}/{max_retries}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"üîå Error de conexi√≥n en intento {intento+1}/{max_retries}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inesperado: {e}\")\n",
    "            \n",
    "        if intento < max_retries - 1:\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"‚ùå Fallaron todos los intentos\")\n",
    "    return None\n",
    "\n",
    "# Probar funci√≥n robusta\n",
    "print(\"\\nüß™ Probando funci√≥n robusta:\")\n",
    "datos = obtener_datos_api(\"https://jsonplaceholder.typicode.com/users/1\")\n",
    "if datos:\n",
    "    print(f\"‚úÖ Datos obtenidos: {datos['name']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ejemplos de APIs completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79b66c",
   "metadata": {},
   "source": [
    "## 5. Automatizaci√≥n de Procesos\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "La **automatizaci√≥n de procesos de datos** consiste en programar la ejecuci√≥n de tareas repetitivas (reportes, descargas, limpiezas, cargas) para que se realicen de forma aut√≥noma, sin intervenci√≥n manual. Utiliza herramientas como scripts Python con `schedule`, planificadores del sistema operativo (Task Scheduler, cron) u orquestadores profesionales como Apache Airflow.\n",
    "\n",
    "**¬øQu√© automatizar?**\n",
    "\n",
    "- **Buenos candidatos**: Reportes peri√≥dicos, descarga de datos de APIs, limpieza y transformaci√≥n, carga a base de datos, env√≠o de alertas, validaci√≥n de calidad, backups\n",
    "- **No automatizar (a√∫n)**: Tareas √∫nicas, procesos que cambian constantemente, an√°lisis exploratorio, tareas que requieren juicio humano\n",
    "\n",
    "> **Regla de tres**: Si haces algo m√°s de 3 veces ‚Üí automat√≠zalo.\n",
    "\n",
    "**Herramientas de automatizaci√≥n:**\n",
    "\n",
    "| Herramienta | Tipo | Complejidad | Uso |\n",
    "|---|---|---|---|\n",
    "| **Scripts Python** | Lo m√°s simple | Baja | Ejecuci√≥n manual o programada |\n",
    "| **schedule** (Python) | Librer√≠a | Baja | Programaci√≥n simple en el script |\n",
    "| **Cron / Task Scheduler** | Sistema operativo | Media | Ejecuci√≥n a horas espec√≠ficas |\n",
    "| **Apache Airflow** | Orquestador | Alta | DAGs, monitoreo, retries, alertas |\n",
    "| **Prefect / Dagster** | Orquestadores modernos | Media | Alternativa pythonic a Airflow |\n",
    "| **AWS Lambda / Cloud Functions** | Cloud serverless | Media | Pago por uso, escala autom√°tica |\n",
    "\n",
    "**Componentes de un proceso automatizado:**\n",
    "\n",
    "1. **Trigger** (Disparador): Tiempo (\"cada d√≠a a las 8 AM\"), evento (\"nuevo archivo\"), o manual\n",
    "2. **Ejecuci√≥n**: El trabajo real (ETL, an√°lisis), con manejo de errores y logging\n",
    "3. **Notificaci√≥n**: √âxito ‚Üí log; Fallo ‚Üí email, Slack, SMS\n",
    "4. **Monitoreo**: Dashboard de estado, historial, alertas de SLA\n",
    "\n",
    "**Mejores pr√°cticas:**\n",
    "\n",
    "- **Idempotencia**: Ejecutar m√∫ltiples veces = mismo resultado\n",
    "- **Manejo de errores**: try/except, logging, reintentos autom√°ticos\n",
    "- **Parametrizaci√≥n**: Configuraci√≥n en archivos externos, no hardcoded\n",
    "- **Testing**: Probar con datos peque√±os antes de producci√≥n\n",
    "- **Documentaci√≥n**: README, comentarios, diagramas de flujo\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Automatizar procesos de datos sirve para:\n",
    "\n",
    "- **Eliminar tareas manuales repetitivas** que consumen tiempo y son propensas a errores\n",
    "- **Garantizar consistencia** ejecutando los mismos pasos de forma id√©ntica cada vez\n",
    "- **Escalar operaciones** a cualquier volumen sin necesidad de m√°s personal\n",
    "- **Liberar tiempo para an√°lisis** dedicando recursos a tareas de mayor valor\n",
    "- **Implementar logging y monitoreo** para saber cu√°ndo algo falla y reaccionar r√°pido\n",
    "- **Programar ejecuciones peri√≥dicas** con reportes diarios, semanales o mensuales autom√°ticos\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, crearemos un script de reporte autom√°tico de ventas con manejo de errores e implementaremos logging profesional para monitoreo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680b8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "               EJEMPLO 1: REPORTE AUTOM√ÅTICO DE VENTAS                \n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "ü§ñ REPORTE AUTOM√ÅTICO DE VENTAS - 2026-02-21\n",
      "============================================================\n",
      "\n",
      "[1/5] üì• Extrayendo datos de ventas...\n",
      "      ‚úì 5 registros extra√≠dos\n",
      "\n",
      "[2/5] üîÑ Transformando datos...\n",
      "      ‚úì Columnas calculadas agregadas\n",
      "\n",
      "[3/5] üìä Generando an√°lisis...\n",
      "      ‚úì M√©tricas calculadas\n",
      "\n",
      "[4/5] üíæ Guardando reporte...\n",
      "      ‚úì Guardado en reporte_ventas_2026-02-21.csv\n",
      "\n",
      "[5/5] üìã Resumen del d√≠a:\n",
      "      ‚Ä¢ Total ventas: $4,675.00\n",
      "      ‚Ä¢ Items vendidos: 29\n",
      "      ‚Ä¢ Ticket promedio: $935.00\n",
      "      ‚Ä¢ Categor√≠a top: Electr√≥nica\n",
      "\n",
      "============================================================\n",
      "‚úÖ REPORTE COMPLETADO EXITOSAMENTE\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "                    EJEMPLO 2: LOGGING PROFESIONAL                    \n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 22:23:47,516 - INFO - ============================================================\n",
      "2026-02-21 22:23:47,518 - INFO - Iniciando proceso automatizado\n",
      "2026-02-21 22:23:47,519 - INFO - ============================================================\n",
      "2026-02-21 22:23:47,520 - INFO - Paso 1: Conectando a base de datos...\n",
      "2026-02-21 22:23:47,821 - INFO - ‚úì Conexi√≥n establecida\n",
      "2026-02-21 22:23:47,822 - INFO - Paso 2: Extrayendo datos...\n",
      "2026-02-21 22:23:48,124 - INFO - ‚úì 1000 registros extra√≠dos\n",
      "2026-02-21 22:23:48,126 - INFO - Paso 3: Procesando datos...\n",
      "2026-02-21 22:23:48,427 - INFO - ‚úì Transformaciones aplicadas\n",
      "2026-02-21 22:23:48,428 - INFO - Paso 4: Generando reporte...\n",
      "2026-02-21 22:23:48,630 - INFO - ‚úì Reporte generado\n",
      "2026-02-21 22:23:48,630 - INFO - ============================================================\n",
      "2026-02-21 22:23:48,631 - INFO - ‚úÖ Proceso completado exitosamente\n",
      "2026-02-21 22:23:48,632 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ejemplos de automatizaci√≥n completados\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO 1: REPORTE AUTOM√ÅTICO DE VENTAS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def generar_reporte_ventas(fecha=None):\n",
    "    \"\"\"Genera reporte autom√°tico de ventas\"\"\"\n",
    "    if fecha is None:\n",
    "        fecha = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü§ñ REPORTE AUTOM√ÅTICO DE VENTAS - {fecha}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. EXTRAER DATOS (simular)\n",
    "        print(\"\\n[1/5] üì• Extrayendo datos de ventas...\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        ventas = pd.DataFrame({\n",
    "            'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Laptop'],\n",
    "            'categoria': ['Electr√≥nica', 'Accesorios', 'Accesorios', 'Electr√≥nica', 'Electr√≥nica'],\n",
    "            'cantidad': [2, 15, 8, 3, 1],\n",
    "            'precio': [1000, 25, 50, 300, 1000],\n",
    "            'fecha': [fecha] * 5\n",
    "        })\n",
    "        print(f\"      ‚úì {len(ventas)} registros extra√≠dos\")\n",
    "        \n",
    "        # 2. TRANSFORMAR\n",
    "        print(\"\\n[2/5] üîÑ Transformando datos...\")\n",
    "        time.sleep(0.3)\n",
    "        ventas['total'] = ventas['cantidad'] * ventas['precio']\n",
    "        print(f\"      ‚úì Columnas calculadas agregadas\")\n",
    "        \n",
    "        # 3. AN√ÅLISIS\n",
    "        print(\"\\n[3/5] üìä Generando an√°lisis...\")\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        total_ventas = ventas['total'].sum()\n",
    "        total_items = ventas['cantidad'].sum()\n",
    "        ticket_promedio = total_ventas / len(ventas)\n",
    "        \n",
    "        print(f\"      ‚úì M√©tricas calculadas\")\n",
    "        \n",
    "        # 4. GUARDAR REPORTE\n",
    "        print(\"\\n[4/5] üíæ Guardando reporte...\")\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        reporte_filename = f'reporte_ventas_{fecha}.csv'\n",
    "        ventas.to_csv(reporte_filename, index=False)\n",
    "        \n",
    "        print(f\"      ‚úì Guardado en {reporte_filename}\")\n",
    "        \n",
    "        # 5. RESUMEN\n",
    "        print(\"\\n[5/5] üìã Resumen del d√≠a:\")\n",
    "        print(f\"      ‚Ä¢ Total ventas: ${total_ventas:,.2f}\")\n",
    "        print(f\"      ‚Ä¢ Items vendidos: {total_items}\")\n",
    "        print(f\"      ‚Ä¢ Ticket promedio: ${ticket_promedio:,.2f}\")\n",
    "        print(f\"      ‚Ä¢ Categor√≠a top: {ventas.groupby('categoria')['total'].sum().idxmax()}\")\n",
    "        \n",
    "        # Limpiar archivo temporal\n",
    "        if os.path.exists(reporte_filename):\n",
    "            os.remove(reporte_filename)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"‚úÖ REPORTE COMPLETADO EXITOSAMENTE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR al generar reporte: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar reporte\n",
    "resultado = generar_reporte_ventas()\n",
    "\n",
    "# EJEMPLO 2: Logging profesional para automatizaci√≥n\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EJEMPLO 2: LOGGING PROFESIONAL\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def proceso_automatizado_con_logging():\n",
    "    \"\"\"Ejemplo de proceso con logging completo\"\"\"\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"Iniciando proceso automatizado\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Paso 1: Conectando a base de datos...\")\n",
    "        time.sleep(0.3)\n",
    "        logger.info(\"‚úì Conexi√≥n establecida\")\n",
    "        \n",
    "        logger.info(\"Paso 2: Extrayendo datos...\")\n",
    "        registros = 1000\n",
    "        time.sleep(0.3)\n",
    "        logger.info(f\"‚úì {registros} registros extra√≠dos\")\n",
    "        \n",
    "        logger.info(\"Paso 3: Procesando datos...\")\n",
    "        time.sleep(0.3)\n",
    "        logger.info(\"‚úì Transformaciones aplicadas\")\n",
    "        \n",
    "        logger.info(\"Paso 4: Generando reporte...\")\n",
    "        time.sleep(0.2)\n",
    "        logger.info(\"‚úì Reporte generado\")\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"‚úÖ Proceso completado exitosamente\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå ERROR en el proceso: {e}\")\n",
    "        logger.exception(\"Traceback completo:\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar\n",
    "proceso_automatizado_con_logging()\n",
    "\n",
    "print(\"\\n‚úÖ Ejemplos de automatizaci√≥n completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e79f55",
   "metadata": {},
   "source": [
    "## 6. Conceptos de Data Warehousing\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "Un **Data Warehouse** es un sistema centralizado de almacenamiento que integra datos de m√∫ltiples fuentes, optimizado para consultas anal√≠ticas y reportes. Se basa en el **modelado dimensional** (esquema estrella o copo de nieve) con tablas de hechos que contienen m√©tricas y tablas de dimensiones que proporcionan contexto descriptivo como productos, clientes, tiempo y ubicaciones.\n",
    "\n",
    "**OLTP vs OLAP:**\n",
    "\n",
    "| Aspecto | OLTP (Transaccional) | OLAP (Anal√≠tico) |\n",
    "|---|---|---|\n",
    "| Prop√≥sito | Operaciones del d√≠a a d√≠a | An√°lisis hist√≥rico |\n",
    "| Operaciones | INSERT, UPDATE, DELETE frecuentes | Principalmente SELECT |\n",
    "| Optimizado para | Escritura | Lectura |\n",
    "| Datos | Actuales | Hist√≥ricos + actuales |\n",
    "| Dise√±o | Normalizado (3NF) | Desnormalizado (estrella) |\n",
    "| Ejemplo | Sistema de ventas, CRM, ERP | Data Warehouse, dashboards |\n",
    "\n",
    "> **OLTP**: \"Registrar esta venta de $100\" ‚Üí **OLAP**: \"¬øCu√°l fue el total de ventas por regi√≥n el a√±o pasado?\"\n",
    "\n",
    "**Esquema Estrella (Star Schema):**\n",
    "\n",
    "La tabla **FACT** (hechos) central contiene m√©tricas num√©ricas (monto, cantidad, descuento) y claves for√°neas hacia las tablas **DIM** (dimensiones) que aportan contexto descriptivo:\n",
    "\n",
    "- **Fact_Ventas**: venta_id, producto_id (FK), cliente_id (FK), fecha_id (FK), cantidad, monto, costo\n",
    "- **Dim_Producto**: producto_id, nombre, categor√≠a, marca, precio_lista\n",
    "- **Dim_Cliente**: cliente_id, nombre, ciudad, segmento\n",
    "- **Dim_Tiempo**: fecha_id, a√±o, mes, d√≠a, trimestre, d√≠a_semana\n",
    "- **Dim_Tienda**: tienda_id, nombre, ciudad, canal\n",
    "\n",
    "**Slowly Changing Dimensions (SCD):**\n",
    "\n",
    "| Tipo | Estrategia | Historial | Ejemplo |\n",
    "|---|---|---|---|\n",
    "| **SCD 0** | No cambiar | Original siempre | Fecha de nacimiento |\n",
    "| **SCD 1** | Sobrescribir | Sin historial | Correcci√≥n de errores |\n",
    "| **SCD 2** | Nueva fila con versi√≥n | Completo | Cliente cambia de ciudad |\n",
    "| **SCD 3** | Columna anterior/actual | Limitado | Solo √∫ltimo cambio |\n",
    "\n",
    "El **SCD Tipo 2** es el m√°s com√∫n: agrega una nueva fila con campos `version`, `fecha_inicio`, `fecha_fin` e `is_current` para rastrear todo el historial.\n",
    "\n",
    "**Plataformas cloud modernas:**\n",
    "\n",
    "- **Snowflake**: Separaci√≥n storage/compute, auto-scaling, pay-per-use\n",
    "- **Google BigQuery**: Serverless, escalable, integraci√≥n Google Cloud\n",
    "- **Amazon Redshift**: Basado en PostgreSQL, clusters configurables\n",
    "- **Azure Synapse**: Integraci√≥n Microsoft, analytics a gran escala\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Comprender data warehousing sirve para:\n",
    "\n",
    "- **Entender la diferencia entre OLTP y OLAP** y por qu√© los sistemas anal√≠ticos se dise√±an diferente\n",
    "- **Dise√±ar modelos dimensionales** con tablas de hechos y dimensiones para an√°lisis eficiente\n",
    "- **Escribir queries m√°s eficientes** al comprender c√≥mo est√°n estructurados los datos en el warehouse\n",
    "- **Manejar cambios hist√≥ricos** con Slowly Changing Dimensions (SCD Tipo 1, 2 y 3)\n",
    "- **Conocer plataformas cloud modernas** como Snowflake, BigQuery, Redshift y Azure Synapse\n",
    "- **Calcular KPIs y m√©tricas** usando la estructura fact-dimension de forma natural y escalable\n",
    "\n",
    "El data warehouse es donde convergen todos los datos procesados para alimentar dashboards, reportes y modelos anal√≠ticos.\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "En el c√≥digo siguiente, crearemos un modelo estrella completo con tablas de dimensiones (producto, cliente, tiempo, tienda) y una tabla de hechos de ventas, para luego realizar an√°lisis por categor√≠a, canal y clientes top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834b3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                  EJEMPLO: MODELO ESTRELLA DE VENTAS                  \n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ CREANDO TABLAS DE DIMENSIONES\n",
      "‚úì Dim_Producto: (5, 5)\n",
      "‚úì Dim_Cliente: (4, 4)\n",
      "‚úì Dim_Tiempo: (10, 6)\n",
      "‚úì Dim_Tienda: (3, 4)\n",
      "\n",
      "2Ô∏è‚É£ CREANDO TABLA DE HECHOS\n",
      "‚úì Fact_Ventas: (50, 9)\n",
      "\n",
      "üìä Primeras filas de Fact_Ventas:\n",
      "   venta_id  producto_id  cliente_id   fecha_id  tienda_id  cantidad    monto  \\\n",
      "0         1            4         104 2024-01-09          1         4   951.20   \n",
      "1         2            5         103 2024-01-09          2         4  1967.18   \n",
      "2         3            3         104 2024-01-01          1         4   809.67   \n",
      "3         4            5         103 2024-01-09          3         1  1636.54   \n",
      "4         5            5         104 2024-01-07          3         4  1600.72   \n",
      "\n",
      "     costo   margen  \n",
      "0   733.56   217.64  \n",
      "1   678.15  1289.03  \n",
      "2  1491.74  -682.07  \n",
      "3   272.13  1364.41  \n",
      "4    36.93  1563.79  \n",
      "\n",
      "======================================================================\n",
      "                 3Ô∏è‚É£ AN√ÅLISIS CON MODELO DIMENSIONAL                  \n",
      "======================================================================\n",
      "\n",
      "üìà Ventas por Categor√≠a de Producto:\n",
      "              Monto Total  Unidades  Num Ventas  Margen Total\n",
      "categoria                                                    \n",
      "Accesorios       39139.72        71          30      15897.87\n",
      "Computadoras      7393.10        21           7        895.08\n",
      "Monitores        14201.33        35          13       4966.46\n",
      "\n",
      "üìä Ventas por Canal:\n",
      "           monto          venta_id\n",
      "             sum     mean    count\n",
      "canal                             \n",
      "F√≠sica  39462.94  1195.85       33\n",
      "Online  21271.21  1251.25       17\n",
      "\n",
      "üë• Top 5 Clientes:\n",
      "                             Monto Total  Num Compras\n",
      "cliente_id nombre_x                                  \n",
      "102        Monitor LG            8148.96            7\n",
      "104        Teclado Mec√°nico      7160.20            5\n",
      "103        Webcam                5319.24            3\n",
      "104        Mouse Logitech        4112.85            4\n",
      "102        Webcam                3767.32            4\n",
      "\n",
      "======================================================================\n",
      "                       üìä M√âTRICAS CLAVE (KPIs)                        \n",
      "======================================================================\n",
      "\n",
      "  üí∞ Total Ventas:      $60,734.15\n",
      "  üìà Total Margen:      $21,759.41\n",
      "  üìä Margen %:          35.8%\n",
      "  üõí Ticket Promedio:   $1,214.68\n",
      "  üì¶ Unidades Vendidas: 127\n",
      "  üî¢ N√∫mero de Ventas:  50\n",
      "\n",
      "‚úÖ Modelo dimensional completado\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EJEMPLO: MODELO ESTRELLA DE VENTAS\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. TABLAS DE DIMENSIONES\n",
    "print(\"\\n1Ô∏è‚É£ CREANDO TABLAS DE DIMENSIONES\")\n",
    "\n",
    "dim_producto = pd.DataFrame({\n",
    "    'producto_id': [1, 2, 3, 4, 5],\n",
    "    'nombre': ['Laptop HP', 'Mouse Logitech', 'Teclado Mec√°nico', 'Monitor LG', 'Webcam'],\n",
    "    'categoria': ['Computadoras', 'Accesorios', 'Accesorios', 'Monitores', 'Accesorios'],\n",
    "    'marca': ['HP', 'Logitech', 'Corsair', 'LG', 'Logitech'],\n",
    "    'precio_lista': [1000, 25, 150, 300, 80]\n",
    "})\n",
    "\n",
    "dim_cliente = pd.DataFrame({\n",
    "    'cliente_id': [101, 102, 103, 104],\n",
    "    'nombre': ['Juan P√©rez', 'Mar√≠a Garc√≠a', 'Carlos L√≥pez', 'Ana Mart√≠nez'],\n",
    "    'ciudad': ['Madrid', 'Barcelona', 'Madrid', 'Valencia'],\n",
    "    'segmento': ['Premium', 'Regular', 'Premium', 'Regular']\n",
    "})\n",
    "\n",
    "dim_tiempo = pd.DataFrame({\n",
    "    'fecha_id': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
    "    'a√±o': 2024,\n",
    "    'mes': 1,\n",
    "    'dia': range(1, 11),\n",
    "    'dia_semana': pd.date_range('2024-01-01', periods=10, freq='D').day_name(),\n",
    "    'trimestre': 1\n",
    "})\n",
    "\n",
    "dim_tienda = pd.DataFrame({\n",
    "    'tienda_id': [1, 2, 3],\n",
    "    'nombre_tienda': ['Tienda Centro', 'Tienda Norte', 'Online'],\n",
    "    'ciudad': ['Madrid', 'Barcelona', 'Online'],\n",
    "    'canal': ['F√≠sica', 'F√≠sica', 'Online']\n",
    "})\n",
    "\n",
    "print(\"‚úì Dim_Producto:\", dim_producto.shape)\n",
    "print(\"‚úì Dim_Cliente:\", dim_cliente.shape)\n",
    "print(\"‚úì Dim_Tiempo:\", dim_tiempo.shape)\n",
    "print(\"‚úì Dim_Tienda:\", dim_tienda.shape)\n",
    "\n",
    "# 2. TABLA DE HECHOS\n",
    "print(\"\\n2Ô∏è‚É£ CREANDO TABLA DE HECHOS\")\n",
    "\n",
    "np.random.seed(42)\n",
    "fact_ventas = pd.DataFrame({\n",
    "    'venta_id': range(1, 51),\n",
    "    'producto_id': np.random.choice(dim_producto['producto_id'], 50),\n",
    "    'cliente_id': np.random.choice(dim_cliente['cliente_id'], 50),\n",
    "    'fecha_id': np.random.choice(dim_tiempo['fecha_id'], 50),\n",
    "    'tienda_id': np.random.choice(dim_tienda['tienda_id'], 50),\n",
    "    'cantidad': np.random.randint(1, 5, 50),\n",
    "    'monto': np.random.uniform(20, 2000, 50).round(2),\n",
    "    'costo': np.random.uniform(10, 1500, 50).round(2)\n",
    "})\n",
    "\n",
    "fact_ventas['margen'] = fact_ventas['monto'] - fact_ventas['costo']\n",
    "\n",
    "print(\"‚úì Fact_Ventas:\", fact_ventas.shape)\n",
    "print(\"\\nüìä Primeras filas de Fact_Ventas:\")\n",
    "print(fact_ventas.head())\n",
    "\n",
    "# 3. AN√ÅLISIS CON MODELO DIMENSIONAL\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ AN√ÅLISIS CON MODELO DIMENSIONAL\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Join fact con dimensiones\n",
    "ventas_completo = (\n",
    "    fact_ventas\n",
    "    .merge(dim_producto, on='producto_id', how='left')\n",
    "    .merge(dim_cliente, on='cliente_id', how='left')\n",
    "    .merge(dim_tienda, on='tienda_id', how='left')\n",
    ")\n",
    "\n",
    "# Ventas por categor√≠a\n",
    "print(\"\\nüìà Ventas por Categor√≠a de Producto:\")\n",
    "ventas_categoria = ventas_completo.groupby('categoria').agg({\n",
    "    'monto': 'sum',\n",
    "    'cantidad': 'sum',\n",
    "    'venta_id': 'count',\n",
    "    'margen': 'sum'\n",
    "}).round(2)\n",
    "ventas_categoria.columns = ['Monto Total', 'Unidades', 'Num Ventas', 'Margen Total']\n",
    "print(ventas_categoria)\n",
    "\n",
    "# Ventas por canal\n",
    "print(\"\\nüìä Ventas por Canal:\")\n",
    "ventas_canal = ventas_completo.groupby('canal').agg({\n",
    "    'monto': ['sum', 'mean'],\n",
    "    'venta_id': 'count'\n",
    "}).round(2)\n",
    "print(ventas_canal)\n",
    "\n",
    "# Top clientes\n",
    "print(\"\\nüë• Top 5 Clientes:\")\n",
    "top_clientes = ventas_completo.groupby(['cliente_id', 'nombre_x']).agg({\n",
    "    'monto': 'sum',\n",
    "    'venta_id': 'count'\n",
    "}).round(2)\n",
    "top_clientes.columns = ['Monto Total', 'Num Compras']\n",
    "print(top_clientes.nlargest(5, 'Monto Total'))\n",
    "\n",
    "# KPIs\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä M√âTRICAS CLAVE (KPIs)\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_ventas = fact_ventas['monto'].sum()\n",
    "total_margen = fact_ventas['margen'].sum()\n",
    "margen_pct = (total_margen / total_ventas * 100)\n",
    "ticket_promedio = fact_ventas['monto'].mean()\n",
    "unidades_totales = fact_ventas['cantidad'].sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "  üí∞ Total Ventas:      ${total_ventas:,.2f}\n",
    "  üìà Total Margen:      ${total_margen:,.2f}\n",
    "  üìä Margen %:          {margen_pct:.1f}%\n",
    "  üõí Ticket Promedio:   ${ticket_promedio:,.2f}\n",
    "  üì¶ Unidades Vendidas: {unidades_totales:,}\n",
    "  üî¢ N√∫mero de Ventas:  {len(fact_ventas):,}\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Modelo dimensional completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f79ce",
   "metadata": {},
   "source": [
    "## Resumen y Pr√≥ximos Pasos\n",
    "\n",
    "### ¬øQu√© es?\n",
    "\n",
    "El **resumen del m√≥dulo** consolida todos los conceptos de ingenier√≠a de datos cubiertos: ETL/ELT, pipelines, formatos de datos, APIs, automatizaci√≥n y data warehousing. Proporciona una visi√≥n integral de c√≥mo estos elementos se conectan en la pr√°ctica profesional y c√≥mo aplicarlos en el rol de analista de datos.\n",
    "\n",
    "**Lo que has aprendido:**\n",
    "\n",
    "| Tema | Conceptos Clave |\n",
    "|---|---|\n",
    "| **ETL vs ELT** | Diferencias entre paradigmas, cu√°ndo usar cada uno, tendencia ELT en cloud |\n",
    "| **Pipelines** | Componentes, batch vs streaming, herramientas (Airflow, schedule) |\n",
    "| **Formatos** | CSV, JSON, Parquet, Avro ‚Äî trade-offs de velocidad, tama√±o y compatibilidad |\n",
    "| **APIs** | Consumir APIs REST, manejo de errores, rate limiting, consideraciones legales |\n",
    "| **Automatizaci√≥n** | Scripts automatizados, logging profesional, Task Scheduler, Airflow |\n",
    "| **Data Warehousing** | OLTP vs OLAP, modelado estrella, SCD, plataformas cloud |\n",
    "\n",
    "**Conexi√≥n con tu rol de analista:**\n",
    "\n",
    "- **Entender de d√≥nde vienen tus datos** y comunicarte mejor con ingenieros\n",
    "- **Ser m√°s aut√≥nomo** creando pipelines simples y automatizando an√°lisis repetitivos\n",
    "- **Mejorar tus an√°lisis** escribiendo queries eficientes sobre modelos dimensionales\n",
    "- **Avanzar en tu carrera** hacia roles h√≠bridos como **Analytics Engineer**\n",
    "\n",
    "**Rol emergente ‚Äî Analytics Engineer:**\n",
    "\n",
    "Combina lo mejor de ambos mundos: transforma datos en el warehouse (dbt), modela datos para an√°lisis, crea m√©tricas centralizadas y documenta/testea modelos. Requiere SQL avanzado, Python, dbt, Git y pensamiento anal√≠tico.\n",
    "\n",
    "**Checklist de autoevaluaci√≥n:**\n",
    "\n",
    "- [ ] Entiendo la diferencia entre ETL y ELT\n",
    "- [ ] S√© qu√© es un pipeline de datos y sus componentes\n",
    "- [ ] Conozco los trade-offs entre CSV, JSON y Parquet\n",
    "- [ ] Puedo consumir APIs con Python y manejar errores\n",
    "- [ ] S√© implementar logging y automatizar procesos\n",
    "- [ ] Entiendo OLTP vs OLAP y el modelado dimensional\n",
    "- [ ] Conozco las plataformas cloud de Data Warehousing\n",
    "\n",
    "> Si marcaste m√°s de 5, ¬°vas muy bien! Si menos de 4, repasa las secciones espec√≠ficas.\n",
    "\n",
    "**Pr√≥ximos pasos sugeridos:**\n",
    "\n",
    "1. **Proyecto ETL Personal**: Elegir API p√∫blica ‚Üí extraer ‚Üí limpiar ‚Üí guardar en Parquet ‚Üí automatizar ‚Üí dashboard\n",
    "2. **Explorar herramientas**: Postman para APIs, free tier de Snowflake/BigQuery, dbt para transformaciones\n",
    "3. **Lecturas recomendadas**: \"Fundamentals of Data Engineering\" (Reis & Housley), \"The Data Warehouse Toolkit\" (Kimball)\n",
    "\n",
    "### ¬øPara qu√© sirve?\n",
    "\n",
    "Este resumen sirve para:\n",
    "\n",
    "- **Repasar los conceptos clave** de cada secci√≥n y verificar tu comprensi√≥n\n",
    "- **Conectar la ingenier√≠a de datos con el rol de analista** entendiendo c√≥mo te hace m√°s vers√°til\n",
    "- **Identificar √°reas de mejora** con el checklist de autoevaluaci√≥n\n",
    "- **Planificar tus pr√≥ximos pasos** con proyectos pr√°cticos y recursos de aprendizaje\n",
    "- **Conocer el rol de Analytics Engineer** como evoluci√≥n natural que combina an√°lisis e ingenier√≠a\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "Revisa el checklist arriba y marca los conceptos que domines. El c√≥digo siguiente muestra un mensaje de finalizaci√≥n del m√≥dulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b15250aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                       ‚úÖ NOTEBOOK 12 COMPLETADO                       \n",
      "======================================================================\n",
      "\n",
      "üìÖ Completado: 2026-02-21 22:24:23\n",
      "\n",
      "üöÄ Has cubierto los fundamentos de Ingenier√≠a de Datos:\n",
      "   1. ETL vs ELT\n",
      "   2. Pipelines de Datos\n",
      "   3. Formatos de Datos\n",
      "   4. APIs y Web Scraping\n",
      "   5. Automatizaci√≥n de Procesos\n",
      "   6. Data Warehousing\n",
      "\n",
      "üí° 'Data is the new oil, but pipelines make it flow.'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ NOTEBOOK 12 COMPLETADO\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÖ Completado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nüöÄ Has cubierto los fundamentos de Ingenier√≠a de Datos:\")\n",
    "print(\"   1. ETL vs ELT\")\n",
    "print(\"   2. Pipelines de Datos\")\n",
    "print(\"   3. Formatos de Datos\")\n",
    "print(\"   4. APIs y Web Scraping\")\n",
    "print(\"   5. Automatizaci√≥n de Procesos\")\n",
    "print(\"   6. Data Warehousing\")\n",
    "\n",
    "print(\"\\nüí° 'Data is the new oil, but pipelines make it flow.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb6f7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Referencias y Recursos Adicionales\n",
    "\n",
    "### üìö Documentaci√≥n Oficial\n",
    "- **Apache Airflow**: https://airflow.apache.org/docs/\n",
    "- **Pandas** (Parquet): https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html\n",
    "- **Requests**: https://docs.python-requests.org/\n",
    "- **Schedule**: https://schedule.readthedocs.io/\n",
    "\n",
    "### üéì Cursos Recomendados\n",
    "- Data Engineering Zoomcamp (DataTalks.Club) - Gratuito\n",
    "- Airflow Fundamentals (Astronomer) - Gratuito\n",
    "- dbt Fundamentals (dbt Labs) - Gratuito\n",
    "\n",
    "### üìñ Libros\n",
    "- \"Fundamentals of Data Engineering\" - Joe Reis & Matt Housley\n",
    "- \"The Data Warehouse Toolkit\" - Ralph Kimball\n",
    "- \"Designing Data-Intensive Applications\" - Martin Kleppmann\n",
    "\n",
    "### üåê APIs P√∫blicas para Practicar\n",
    "- https://github.com/public-apis/public-apis\n",
    "- https://jsonplaceholder.typicode.com/\n",
    "- https://rapidapi.com/\n",
    "\n",
    "### üõ†Ô∏è Herramientas\n",
    "- **Airflow**: Orquestaci√≥n de pipelines\n",
    "- **dbt**: Transformaci√≥n de datos en warehouse\n",
    "- **Prefect**: Alternativa moderna a Airflow\n",
    "- **Great Expectations**: Validaci√≥n de calidad de datos\n",
    "\n",
    "### üë• Comunidades\n",
    "- r/dataengineering\n",
    "- dbt Community Slack\n",
    "- Data Engineering Weekly Newsletter\n",
    "- Locally Optimistic (Blog)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook desarrollado como parte de la Ruta de Analista de Datos con Python**\n",
    "\n",
    "*√öltima actualizaci√≥n: Febrero 2026*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da_basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
